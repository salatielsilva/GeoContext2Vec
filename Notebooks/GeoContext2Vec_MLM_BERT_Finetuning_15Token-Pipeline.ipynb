{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde4b366",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63250ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4a409",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddda663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee038d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dataset do torch auxilia no treinamento dos modelos\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c95d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function converts the list of sentences into a BERT input\n",
    "\"\"\"\n",
    "def bertInput_clean(sentences):\n",
    "\n",
    "    token_text = \"[SEP]\".join(sentences)\n",
    "    \n",
    "    return token_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function converts the list of sentences into a BERT input\n",
    "\"\"\"\n",
    "def input_clean(sentences):\n",
    "\n",
    "    token_text = \" \".join(sentences)\n",
    "    \n",
    "    return token_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d535deb",
   "metadata": {},
   "source": [
    "## Split by Geographic Feature Version\n",
    "\n",
    "- This version separates the data considering the replicated geographic data types.\n",
    "\n",
    "Whenever the geographic data changes, it signifies that another replication sequence will start.- \n",
    "There's also a sentence size control to prevent tokenization overflow- .\n",
    "In this case, small documents based on these changes are generate- d.\n",
    "The validation set is created using an 80-20 spl- it.\n",
    "It doesn't work with Early Stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a97e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados...\n",
      "./geographic/GEOC2VEC/austin-sl-tuple-geoc2vec-0bins_polygons_information-wgt0.8pfp-c.parquet\n",
      "Quantidade de sentenças: 1822380\n",
      "Gerando subtextos com foco nos tipos de POI...\n",
      "16\n",
      "Gerando conjunto de Treino e Validação...\n",
      "Conjunto de Treino:  70503\n",
      "Conjunto de Validação:  17626\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "SENTENCE_SIZE = 200\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "MAX_LEN = 512\n",
    "MASK_PERC = 0.15\n",
    "LR = 5e-5\n",
    "valid_size = 0.2\n",
    "patience = 5\n",
    "use_amp = True\n",
    "\n",
    "weights = [0.0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "osm_tables = ['bins_points_information', 'bins_polygons_information', 'bins_roads_information', 'bins_lines_information']\n",
    "\n",
    "for n in range(0, 1):\n",
    "    \n",
    "\n",
    "    for w in weights:\n",
    "        #Adjusting parameter w\n",
    "        wgt = round(w, 1)\n",
    "        for osm_table in osm_tables:\n",
    "        \n",
    "            #Flag to allow correct training\n",
    "            do_training = False\n",
    "            \n",
    "            #Special case to load point data and train only once\n",
    "            if(osm_table == 'bins_points_information' and wgt == 0.0):\n",
    "                file_name = './austin-sl-tuple-geoc2vec-' + str(n) + osm_table + '-pfp-c.csv'\n",
    "                model_name = './austin-sl-tuple-geoc2vec-distilbert-MLM-' + str(n) + osm_table + '-pfp-c'\n",
    "                \n",
    "                do_training = True\n",
    "                \n",
    "            elif(osm_table != 'bins_points_information'):\n",
    "                \n",
    "                file_name = './austin-sl-tuple-geoc2vec-' + str(n) + osm_table + '-wgt' + str(wgt) + 'pfp-c.csv'\n",
    "                model_name = './austin-sl-tuple-geoc2vec-distilbert-MLM-' + str(n) + osm_table + '-wgt' + str(wgt) + '-pfp-c'\n",
    "                do_training = True\n",
    "                \n",
    "            if(do_training):\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                #Loading the empty model for fine-tuning\n",
    "                model = DistilBertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "                #Load dataset\n",
    "                print(\"Loading data...\")\n",
    "                print(file_name)\n",
    "                sentences = pd.read_csv(file_name)\n",
    "                sentences = sentences.values.tolist()\n",
    "                print('Number of sentences:', len(sentences))\n",
    "\n",
    "\n",
    "                print(\"Generating subtexts focusing on POI types...\")\n",
    "                #Creating smaller texts with the set of two words from POIs and geographical data\n",
    "                #This method considers the change of the geographical data TYPE to create a new subtext\n",
    "                bert_sentences = []\n",
    "                local_sentences = []\n",
    "                actual_type = sentences[0][3] # => Geographical data\n",
    "                for i, sentence in enumerate(sentences):\n",
    "\n",
    "                    if(sentence[3] != actual_type):\n",
    "                        \n",
    "                        final_sentence = '[SEP]'.join(local_sentences)\n",
    "                        bert_sentences.append(final_sentence)\n",
    "                        \n",
    "                        actual_type = sentence[3]\n",
    "                        sentence_text = [sentence[1], sentence[3]]\n",
    "                        local_sentences = [input_clean(sentence_text)]\n",
    "\n",
    "                    else:\n",
    "                        sentence_text = [sentence[1], sentence[3]]\n",
    "                        local_sentences.append(input_clean(sentence_text))\n",
    "\n",
    "                #Adding last segment\n",
    "                if(len(local_sentences) > 0):\n",
    "                    bert_sentences.append(final_sentence)\n",
    "            \n",
    "                #Clearing memory\n",
    "                del sentences, local_sentences\n",
    "                \n",
    "                print(\"Generating Training and Validation sets...\")\n",
    "                train, validation = train_test_split(bert_sentences, test_size=valid_size, random_state=42)\n",
    "                print('Training set: ', len(train))\n",
    "                print('Validation set: ', len(validation))\n",
    "                \n",
    "                \n",
    "                #Clearing memory\n",
    "                del bert_sentences\n",
    "\n",
    "                #Tokenizing and saving a copy of the tokens to represent the labels\n",
    "                inputs_train = tokenizer(train, return_tensors='pt', max_length=MAX_LEN, truncation = True, padding=True)\n",
    "                inputs_train['labels'] = inputs_train.input_ids.detach().clone()\n",
    "\n",
    "                inputs_val = tokenizer(validation, return_tensors='pt', max_length=MAX_LEN, truncation = True, padding=True)\n",
    "                inputs_val['labels'] = inputs_val.input_ids.detach().clone()\n",
    "\n",
    "\n",
    "                #Clearing memory\n",
    "                del train, validation\n",
    "\n",
    "                #Finding MASK_PERC% of sentences to be masked\n",
    "                #The sentences will have the second sentence masked e.g., [CLS]Bar[SEP]Park[SEP] => [CLS]Bar[SEP]#####[SEP]\n",
    "                print(\"Masking data...\")\n",
    "                rand_train = torch.rand(inputs_train.input_ids.shape)\n",
    "                rand_val = torch.rand(inputs_val.input_ids.shape)\n",
    "                \n",
    "                #Generating random masking positions\n",
    "                #101 = [CLS]\n",
    "                #102 = [SEP]\n",
    "                mask_arr_train = (rand_train < MASK_PERC) * (inputs_train.input_ids != 101) * \\\n",
    "                           (inputs_train.input_ids != 102) * (inputs_train.input_ids != 0)\n",
    "                \n",
    "                mask_arr_val = (rand_val < MASK_PERC) * (inputs_val.input_ids != 101) * \\\n",
    "                           (inputs_val.input_ids != 102) * (inputs_val.input_ids != 0)\n",
    "                \n",
    "                #Tokens to be masked\n",
    "                selection_train = []\n",
    "                for i in range(inputs_train.input_ids.shape[0]):\n",
    "                    selection_train.append(\n",
    "                        torch.flatten(mask_arr_train[i].nonzero()).tolist()\n",
    "                    )\n",
    "                    \n",
    "                selection_val = []\n",
    "                for i in range(inputs_val.input_ids.shape[0]):\n",
    "                    selection_val.append(\n",
    "                        torch.flatten(mask_arr_val[i].nonzero()).tolist()\n",
    "                    )\n",
    "\n",
    "                #Clearing memory\n",
    "                del rand_train, mask_arr_train, rand_val, mask_arr_val\n",
    "                \n",
    "                #Changing tokens\n",
    "                #103 = [MASK]\n",
    "                for i in range(inputs_train.input_ids.shape[0]):\n",
    "                    inputs_train.input_ids[i, selection_train[i]] = 103\n",
    "                \n",
    "                for i in range(inputs_val.input_ids.shape[0]):\n",
    "                    inputs_val.input_ids[i, selection_val[i]] = 103\n",
    "                    \n",
    "                #Clearing memory\n",
    "                del selection_train, selection_val\n",
    "\n",
    "                #Transforming data into torch dataset object\n",
    "                print(\"Preparing for training...\")\n",
    "                dataset_train = Dataset(inputs_train)\n",
    "                loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "                dataset_val = Dataset(inputs_val)\n",
    "                loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "                \n",
    "                #Clearing memory\n",
    "                del inputs_train, inputs_val\n",
    "\n",
    "                #Preparing the device for training\n",
    "                device = 'cuda' if cuda.is_available() else 'cpu' # CPU OR GPU\n",
    "                torch.cuda.empty_cache()\n",
    "                # and move our model over to the selected device\n",
    "                model.to(device)\n",
    "\n",
    "                optim = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "                # to track the training loss as the model trains\n",
    "                train_losses = []\n",
    "                # to track the validation loss as the model trains\n",
    "                valid_losses = []\n",
    "                # to track the average training loss per epoch as the model trains\n",
    "                avg_train_losses = []\n",
    "                # to track the average validation loss per epoch as the model trains\n",
    "                avg_valid_losses = [] \n",
    "\n",
    "                # initialize the early_stopping object\n",
    "                early_stopping = EarlyStopping(patience=patience, verbose=False)\n",
    "                scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "                has_early_stopping = False\n",
    "\n",
    "                for epoch in range(EPOCHS):\n",
    "                    # setup loop with TQDM and dataloader\n",
    "                    # activate training mode\n",
    "                    model.train()\n",
    "                    loop_train = tqdm(loader_train, leave=True)\n",
    "                    for batch_train in loop_train:\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        # pull all tensor batches required for training\n",
    "                        input_ids = batch_train['input_ids'].to(device)\n",
    "                        attention_mask = batch_train['attention_mask'].to(device)\n",
    "                        labels = batch_train['labels'].to(device)\n",
    "\n",
    "                        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                                        labels=labels)\n",
    "                            loss = outputs.loss\n",
    "\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optim)\n",
    "                        scaler.update()\n",
    "\n",
    "                        loop_train.set_description(f'Epoch {epoch}')\n",
    "                        loop_train.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "                        train_losses.append(loss.item())\n",
    "\n",
    "                    ######################    \n",
    "                    # validate the model #\n",
    "                    ######################\n",
    "                    model.eval() # prep model for evaluation\n",
    "                    loop_val = tqdm(loader_val, leave=True)\n",
    "                    for batch_val in loop_val:\n",
    "\n",
    "                        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                        input_ids = batch_val['input_ids'].to(device)\n",
    "                        attention_mask = batch_val['attention_mask'].to(device)\n",
    "                        labels = batch_val['labels'].to(device)\n",
    "\n",
    "                        # process\n",
    "                        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                                        labels=labels)\n",
    "                            loss = outputs.loss\n",
    "\n",
    "                        # record validation loss\n",
    "                        valid_losses.append(loss.item())\n",
    "\n",
    "                    # calculate average loss over an epoch\n",
    "                    train_loss = np.average(train_losses)\n",
    "                    valid_loss = np.average(valid_losses)\n",
    "                    avg_train_losses.append(train_loss)\n",
    "                    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "                    # clear lists to track next epoch\n",
    "                    train_losses = []\n",
    "                    valid_losses = []\n",
    "\n",
    "                #Saving the trained model\n",
    "                print(\"Saving the model...\")\n",
    "                args = TrainingArguments(\n",
    "                    output_dir=model_name,\n",
    "                    per_device_train_batch_size=BATCH_SIZE,\n",
    "                    num_train_epochs=EPOCHS\n",
    "                )\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=args,\n",
    "                    train_dataset=dataset_train,\n",
    "                    eval_dataset=dataset_val\n",
    "                )\n",
    "\n",
    "                trainer.save_model()\n",
    "                \n",
    "                final_time = (time.time() - start_time)\n",
    "                training_dictionary = {'epoch': (EPOCHS+1),\n",
    "                                       'epochs': EPOCHS,\n",
    "                                       'patience': patience,\n",
    "                                       'train_loss': train_loss,\n",
    "                                       'valid_loss': valid_loss,\n",
    "                                       'avg_train_losses':avg_train_losses,\n",
    "                                       'avg_valid_losses': avg_valid_losses,\n",
    "                                       'time': final_time}\n",
    "                \n",
    "                #Saving training statistics\n",
    "                file_name = model_name + '/training_dictionary.json'\n",
    "                with open(file_name, \"w\") as outfile:\n",
    "                    json.dump(training_dictionary, outfile)\n",
    "                \n",
    "                #Clearing memory\n",
    "                del loader_train\n",
    "                del loader_val\n",
    "                del dataset_train\n",
    "                del dataset_val\n",
    "                del train_losses\n",
    "                del valid_losses\n",
    "                del avg_train_losses\n",
    "                del avg_valid_losses\n",
    "                del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb1776-8bab-4d41-8b53-2a5884cc07ab",
   "metadata": {},
   "source": [
    "# Getting Embeddings Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains embeddings of sentences instead of words\n",
    "def executeModel_CLS(model, tokenizer, poi_type):\n",
    "    \n",
    "    # Tokenizes the text\n",
    "    tokenized_text = tokenizer(poi_type, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(**tokenized_text, output_hidden_states=True)\n",
    "    \n",
    "    # Create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(encoded_layers['hidden_states'], dim=0)\n",
    "    \n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    \n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.mean(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec.tolist())\n",
    "        \n",
    "\n",
    "    # print ('Shape is: %d x %d' % (len(token_vecs_sum_text_01), len(token_vecs_sum_text_01[0])))\n",
    "    \n",
    "    # CLS vector\n",
    "    return token_vecs_sum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd7e9d-8ece-430d-a1ee-7fe30b9ef8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "wgt = 0.5\n",
    "\n",
    "# File paths for different types of geographic information\n",
    "file_name_point = './austin-sl-tuple-geoc2vec-distilbert-MLM-' + str(n) + 'bins_points_information-pfp-c'\n",
    "file_name_polygon = './austin-sl-tuple-geoc2vec-distilbert-MLM-' + str(n) + 'bins_polygons_information-wgt' + str(wgt) + '-pfp-c'\n",
    "file_name_roads = './austin-sl-tuple-geoc2vec-distilbert-MLM-' + str(n) + 'bins_roads_information-wgt' + str(wgt) + '-pfp-c'\n",
    "file_name_lines = './austin-sl-tuple-geoc2vec-distilbert-MLM-' + str(n) + 'bins_lines_information-wgt' + str(wgt) + '-pfp-c'\n",
    "\n",
    "# Load pre-trained models for different types of geographic information\n",
    "model_points = DistilBertForMaskedLM.from_pretrained(file_name_point)\n",
    "model_polygons = DistilBertForMaskedLM.from_pretrained(file_name_polygon)\n",
    "model_roads = DistilBertForMaskedLM.from_pretrained(file_name_roads)\n",
    "model_lines = DistilBertForMaskedLM.from_pretrained(file_name_lines)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model_points.eval()\n",
    "model_lines.eval()\n",
    "model_roads.eval()\n",
    "model_polygons.eval()\n",
    "            \n",
    "poi_type = 'Park'\n",
    "\n",
    "# Obtaining the embeddings for each word\n",
    "sentence_embedding_points = executeModel_CLS(model_points, tokenizer, poi_type)\n",
    "sentence_embedding_lines = executeModel_CLS(model_lines, tokenizer, poi_type)\n",
    "sentence_embedding_roads = executeModel_CLS(model_roads, tokenizer, poi_type)\n",
    "sentence_embedding_polygons = executeModel_CLS(model_polygons, tokenizer, poi_type)\n",
    "\n",
    "# Concatenating the embeddings\n",
    "poi_type_embedding = sentence_embedding_points + sentence_embedding_lines + sentence_embedding_roads + sentence_embedding_polygons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
